---
layout:     post
title:      BLOGS
subtitle:   ""
date:       2021-01-01 # Don't change this one, it's a key for mapping
author:     ""
header-img: img/nyupurple.jpg
# catalog: true
tags:
    - Blogs
permalink:  /blogs/iml-gnn
---

# Graph Convolution Networks (GCN): A Gentle Introduction

## Why do we need another _ _ N?

In previous lectures, we have discussed Convolutional Neural Networks (CNN), which have sort of transformed the world already. We learned that CNN can be used for a variety of problems which involve images. Basically, if you are using anything that processes images, then there is probably a CNN operating on top of the images.

So the question is that if we already have architectures that are powerful, why do people bother coming up with a new approach?

The thing is that CNN is designed to exploit some structure in the data, and we are sort of enforcing this notion of locality. The kernel is looking at a $5 \times 5$ or $3 \times 3$ space and then sliding it over to the next. So it is saying that there is interesting information in the locality of this $5 \times 5$ space.

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206165705631.png" style="width:100%">
    <figcaption align = "center">
        <b>CNN on an image</b>
        <p>Source: <a href="http://snap.stanford.edu/class/cs224w-2019/slides/08-GNN.pdf">Jure Leskovec, Stanford CS224W: Machine Learning with Graphs</a></p>
    </figcaption>
</figure>

Natural images have structures in it, and these structures are easy to extract using a CNN. However, if the image loses such locality, for example, if we randomly permute pixels inside images and train a CNN on top of them, it will not work as well because the images are losing locality information that the CNN tries to pick up. You can read more about this on this paper that discuss [Convolutional Neural Networks on Randomized Data](https://arxiv.org/abs/1907.10935v1).

<figure>
    <img src="/img/blogs/iml-gnn/image-pixelspermutation.png" style="width:100%">
    <figcaption align = "center">
        <b>Pixel-wise Randomization of MNIST Natural Images</b>
        <p>Source: <a href="https://arxiv.org/abs/1907.10935v1">C. Ivan, ‘Convolutional Neural Networks on Randomized Data’, 2019.</a></p>
    </figcaption>
</figure>

While CNN can perform really well on structured data like images, speech data, and grid games, many of the real-life data does not have this explicit structure that CNN can easily pick up on.

Many of the problems and datasets have a graph structure. For example, social networks, citation networks, communication networks, protein interaction networks, road maps, etc. 

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206170515434.png" style="width:100%">
    <figcaption align = "center">
        <b>Dataset with Graph Structure</b>
        <p>Source: Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning</p>
    </figcaption>
</figure>

It is very difficult to perform CNN on graphs because of the arbitrary size of the graph, and the complex topology, which means there is no spatial locality. 

And that is why GCN is here to help!

## How it works?

### Graph Example: Facebook

Let's take the graph of Facebook users as an example.

In this graph, each node represents features of a user (e.g., age, race, post history, ...), and each edge is showing if the two users are friends or not.

Our label output $y$ is that if the user will vote for the Democrats ($y=1$) or Republicans ($y=0$).

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206204945554.png" style="width:100%">
    <figcaption align = "center">
        <b>A Graph Representation of Facebook Users</b>
        <p>Node: Features of a User (age, race, location, post history, ...)</p>
        <p>Edge: Friends or Not</p>
        <p>y: Will the user vote for Democrats?</p>
        <p>Source: Figure Drawn by Author</p>
    </figcaption>
</figure>

For example, by looking at the post history of users in purple nodes, we know that which party they will vote for. However, the blue user in the middle does not have much post history, and we want to make a prediction on whether the user is going to vote for Democrats for not.

In this case, we can look at the features of the blue user's friends. If all of their friends are voting for Democrats then we can sort of assume that the blue user will vote for Democrats as well.

We can also take a step further. Let's say if the purple user also does not have a post history. Then we can look at the friends of the purple user.

So how do we bring this idea into a neural network? If I have a node, and if I know what this node is connected to, how I can then use this information of connectivity, and transfer that information to this initial node?

### Visualization of Network

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206211306533.png" style="width:100%">
    <figcaption align = "center">
        <b>Visualization of GNN</b>
        <p>Source: Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning</p>
    </figcaption>
</figure>

Suppose we have a input dataset of a graph with $N$ nodes (users) and each node has $F$ features (e.g., age, race, post history, ...). Then our dataset size is $N \times F$.

At each hidden layer, the graph neural network will always have $N$ nodes.

In the first hidden layer, we will update each of these nodes with the information of its nearby nodes. After that we will feed it through a non-linearity, like a Sigmoid or a ReLU, and repeat this process over and over again.

At the input layer, we have input size of $N \times F$. After the first hidden layer, we will get another dimension of $N \times L_1$, where $L_1$ is the feature size of each node.

At the end, we will have a output of size $N \times d$ where $d$ is the dimensionality of the output. In our Facebook users voting prediction example, since we are doing a classification, we just want to output which class it is, so $d=1$.

### Viewing CNN as a Special Case of GNN

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206213730657.png" style="width:100%">
    <figcaption align = "center">
        <b>CNN: a Special Case of GNN</b>
        <p>Source: Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning</p>
    </figcaption>
</figure>

We can actually view CNN as a special case of GNN.

For example, we have an image, and we are looking at one pixel if the middle of this $3 \times 3$ kernel. At the next layer, we are taking the information of all the other 8 nearby pixels. And then update the value of the middle pixel.

So, CNN is in some sort, a GNN applied on the image, where the node is only connected in a $3 \times 3$ neighborhood.

Now when we are making an update, we can view that as a message passing where we are transforming the messages from the previous layer. And we are adding them up after multiplying with some weights. After that, we apply some non-linearity on top of it.

### Taking a Closer Look

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206220211809.png" style="width:100%">
    <figcaption align = "center">
        <b>Update Rule</b>
        <p>Source: Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning</p>
    </figcaption>
</figure>

At each update at each layer, for each node, we are going to look at the features for all its adjacent nodes and update the feature for that specific node.

Let's denote the red node at node $h_i$. The red node's features at layer $L$ is denoted by $h_i^{(l)}$. And the red node's features at layer $L+1$ is denoted by $h_i^{(l+1)}$.

When we are updating the red node's features at layer $L+1$, we first look at all the nodes in the neighborhood of that node $i$, which are the 4 nodes connected to it. We sum over that and multiply that with some weight $W_1^{(l)}$. Then, we divide that with some amount of constant, so that if we have a large number of weight, we can make sure that the activation is not exploding.

It is interesting to note that in this operation, there are only 2 weights being applied: $W_0^{(l)}$ and $W_1^{(l)}$.

- $W_0^{(l)}$ is saying how much of the node's own feature is being transmitted from one layer to the next layer.
- $W_1^{(l)}$ is saying how much of the nearby neighbors is going to contribute to the next layers features. This  one weight is being applied to all of the neighbor nodes.

There are some great properties:

- Weight sharing over all locations: the $W_1^{(l)}$ weight that we discussed above
- Invariance to permutations: since the process is only looking at the neighbors, it does not matter which node appears where, as long as its neighbor adjacency stays the same.

See this Jupyter Notebook for a code example of [Node classification with Graph Convolutional Network (GCN)](https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html#).

### Can we learn the edges as well?

Taking the Facebook example again, a user may be friends with 1000 other users. However, using this structure cannot tell us who are the close friends and who are just the acquaintances.

So want to think about how can we create neural network which can not just give a fixed number of adjacency but also include edge information as well.

<figure>
    <img src="/img/blogs/iml-gnn/image-20221206223601904.png" style="width:100%">
    <figcaption align = "center">
        <b>Update Rule</b>
        <p>Source: Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning</p>
    </figcaption>
</figure>

We will not be going over a lot of details here.

The basic idea is that if the features of 2 nodes are similar, then the edges should have some information saying that the nodes are similar.

So we take the node features and try to figure out what the edge features are going to be. And once we have the edge features, we can put a GCN on it and go from the edge features back to node features.

## References

[1] [Lerrel Pinto, NYU CSCI-UA 473 Introduction to Machine Learning](https://nyu-robot-learning.github.io/ml-class/)

[2] [Graph Neural Network and Some of GNN Applications: Everything You Need to Know](https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications)

[3] [GRAPH CONVOLUTIONAL NETWORKS](https://tkipf.github.io/graph-convolutional-networks/)

[4] [Jure Leskovec, Stanford CS224W: Machine Learning with Graphs](http://snap.stanford.edu/class/cs224w-2019/slides/08-GNN.pdf)

